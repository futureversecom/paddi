{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To create a virtual environment and install dependencies, run: `pants venv`. This creates a virtual environment with all the needed deps. Then activate the virtual environment by runnin `dist/export/python/virtualenvs/python-default/3.8.16/bin/source`.\n",
        "\n",
        "The next few blocks set-up python paths on dependent modules and imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"../../schemas/gen/py\")\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gym.wrappers import TimeLimit\n",
        "import gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo.policies import MultiInputPolicy\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common import results_plotter\n",
        "from tqdm.rich import trange\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "from pong.env.rewards import RewardConfig\n",
        "from pong.env.pong_env import PongEnv, EnvConfig, PaddleConfig\n",
        "from pong.env.opponent import StaticOpponent, ModelOpponent\n",
        "from pong.env.spaces import ActionType\n",
        "from pong.renderer.pygame_renderer import PygameRenderer\n",
        "from pong.renderer.event_renderer import EventRenderer, FileEventWriter\n",
        "import pong.services.evaluation_runner import EvaluationRunner, WrappedEnv\n",
        "from pong.simulation.config import SimulationConfig\n",
        "from pong.env.additional_logs_callback import AdditionalLogsCallback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Shared variables and utils\n",
        "Execute this block to allow partial execution of the notebook from a cell bellow\n",
        "\n",
        "### Baseline player config\n",
        "\n",
        "The baseline player has genome attributes that puts it at an advantage over most other as we want a good performing agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shared variables and configurations that could be used downstream. We co-locate these so this block could be executed\n",
        "# and partial execution from a cell bellow works.\n",
        "\n",
        "width = 800\n",
        "height = 400\n",
        "\n",
        "# Change this to use a clean output dir\n",
        "experiment_id = 3\n",
        "experiment = f\"baseline_heavy/{experiment_id}\"\n",
        "\n",
        "\n",
        "\n",
        "log_dir = f\".logs/{experiment}\"\n",
        "tb_log_dir = f\".logs/{experiment}_tb\"\n",
        "\n",
        "os.makedirs(experiment, exist_ok = True)\n",
        "os.makedirs(tb_log_dir, exist_ok = True)\n",
        "os.makedirs(f\".models/{experiment}\", exist_ok = True)\n",
        "\n",
        "\n",
        "def pygame_renderer() -> PygameRenderer:\n",
        "    return PygameRenderer(width, height, 60)\n",
        "\n",
        "def model_dir_for(name: str) -> str:\n",
        "    return f\".models/{experiment}/{name}\"\n",
        "\n",
        "def load_model(name: str) -> PPO:\n",
        "    model = PPO.load(model_dir_for(name))\n",
        "    model.tensorboard_log = tb_log_dir\n",
        "    return model\n",
        "\n",
        "def load_baseline_model() -> PPO:\n",
        "    return PPO.load(\"../models/balanced_opponent.zip\")\n",
        "\n",
        "def load_baseline_collab_model() -> PPO:\n",
        "    return PPO.load(\"../models/balanced_collab_opponent.zip\")\n",
        "\n",
        "def evaluate(model: PPO, env: gym.Env, config: SimulationConfig, iterations = 100):\n",
        "    EvaluationRunner(pygame_renderer).evaluate(model, env, config, 100)\n",
        "    \n",
        "def load_and_evaluate(name: str, env: PongEnv, config: SimulationConfig, iterations = 100):\n",
        "    model = load_model(name)\n",
        "    model.set_env(env)\n",
        "    env.enable_training()\n",
        "\n",
        "    EvaluationRunner(pygame_renderer()).evaluate(model, env, config, 100)\n",
        "    \n",
        "def load_and_evaluate_models(name_l: str, name_r: str, env: PongEnv, config: SimulationConfig, iterations = 100):\n",
        "    model_l = load_model(name_l)\n",
        "    model_r = load_model(name_r)\n",
        "\n",
        "    env.set_opponent(ModelOpponent(model_r))\n",
        "    env.enable_training()\n",
        "#     model_l.set_env(env)\n",
        "\n",
        "    EvaluationRunner(pygame_renderer()).evaluate(model_l, env, config, 100)\n",
        "\n",
        "# helper for running the training loop but keep checkpointing along the way\n",
        "def learn(model: PPO, name: str, env: PongEnv, iterations: int):\n",
        "    wrapped = TimeLimit(env, 60 * 60 * 1)\n",
        "    model.set_env(wrapped)\n",
        "    \n",
        "    # store every 10k steps\n",
        "    for r in range(0, iterations, 10_000):\n",
        "        model.learn(total_timesteps=10_000, tb_log_name=name, callback=AdditionalLogsCallback(), reset_num_timesteps=False)\n",
        "        model.save(model_dir_for(name))\n",
        "    \n",
        "    \n",
        "    \n",
        "# =========== Baseline player config ==============\n",
        "# The baseline player has genome attributes that puts it at an advantage over most other as we want a\n",
        "# good performing agent. \n",
        "player_paddle = PaddleConfig(width = 150, strength = 10, endurance=2, max_speed=200,)\n",
        "\n",
        "# A static paddle that does not move\n",
        "wall_paddle = PaddleConfig(width = 1000, strength = 1000, endurance=3, max_speed=500,)\n",
        "\n",
        "# The baseline paddle trained with a better than usual genome.\n",
        "baseline_paddle = PaddleConfig(width = 100, strength = 25, endurance=10, max_speed=1000,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Play against the wall\n",
        "\n",
        "Start with a completely random model and train against a wall as we don't have any other baseline, This way the agents learns to follow the ball and defend on the early stages.\n",
        "Reward for near misses as this is a faster way for the agent to learn to "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "reward_config = RewardConfig(\n",
        "    win_round = 10,\n",
        "    lose_round = -5,\n",
        "    paddle_hit = 5,\n",
        "    near_miss_multiplier = 1,\n",
        "    near_miss_exponent = 5,\n",
        "    near_miss_min_distance = height / 2,\n",
        "    survival_reward_multiplier = 1,\n",
        "    endurance_penalty_multiplier = -1,\n",
        ")\n",
        "\n",
        "play_against_wall_config = EnvConfig(\n",
        "    paddle_l = player_paddle,\n",
        "    paddle_r = wall_paddle,\n",
        "    width = width,\n",
        "    height = height,\n",
        ")\n",
        "\n",
        "play_against_wall_env = PongEnv(play_against_wall_config, reward_config)\n",
        "play_against_wall_env.enable_training()\n",
        "play_against_wall_env.set_opponent(StaticOpponent(ActionType.STOP, 0.0))\n",
        "\n",
        "model = PPO(MultiInputPolicy, play_against_wall_env, verbose=0, device=\"cpu\", tensorboard_log=tb_log_dir, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "learn(model, \"wall_near_miss\", play_against_wall_env, 1_000_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(model_dir_for(\"wall_near_miss\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mload_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwall_near_miss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplay_against_wall_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplay_against_wall_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[20], line 40\u001b[0m, in \u001b[0;36mload_and_evaluate\u001b[0;34m(name, env, config, iterations)\u001b[0m\n\u001b[1;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39mset_env(env)\n\u001b[1;32m     38\u001b[0m env\u001b[38;5;241m.\u001b[39menable_training()\n\u001b[0;32m---> 40\u001b[0m \u001b[43mEvaluationRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpygame_renderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/projects/asm/acp-demo/packages/pong-simulation/src/services/evaluation_runner.py:60\u001b[0m, in \u001b[0;36mEvaluationRunner.evaluate\u001b[0;34m(self, model, env, config, iterations)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Render the current state of the simulation, via the environment\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevents\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrewards\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m frame_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     65\u001b[0m result \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m~/projects/asm/acp-demo/packages/pong-simulation/src/renderer/pygame_renderer.py:132\u001b[0m, in \u001b[0;36mPygameRenderer.render\u001b[0;34m(self, state, events, rewards, config, frame_index)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_surface()\u001b[38;5;241m.\u001b[39mblit(score_text, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle_x \u001b[38;5;241m-\u001b[39m score_text\u001b[38;5;241m.\u001b[39mget_rect()\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m    131\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "load_and_evaluate(\"wall_near_miss\", play_against_wall_env, play_against_wall_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Play against itself to improve on defense and learn attacking moves\n",
        "\n",
        "The use the same model to play with it-self. The wall helps to learn defense but does not allow to learn attacking strategies. Lest setup two reward confics so we could get two model behaviors, one model that is rewarded for defense and another to attack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = load_model(\"wall_near_miss\")\n",
        "baseline_model = load_baseline_collab_model()\n",
        "\n",
        "# still no reward for winning, we need to defend a moving target first\n",
        "reward_config = RewardConfig(\n",
        "    win_round = 10,\n",
        "    lose_round = -10,\n",
        "    paddle_hit = 5,\n",
        "    near_miss_multiplier = 0,\n",
        "    near_miss_exponent = 5,\n",
        "    near_miss_min_distance = height / 10,\n",
        "    survival_reward_multiplier = 2,\n",
        "    endurance_penalty_multiplier = -1,\n",
        ")\n",
        "\n",
        "env_config = EnvConfig(\n",
        "    paddle_l = player_paddle,\n",
        "    paddle_r = baseline_paddle,\n",
        "    width = width,\n",
        "    height = height,\n",
        ")\n",
        "\n",
        "self_env = PongEnv(env_config, reward_config)\n",
        "self_env.enable_training()\n",
        "self_env.set_opponent(ModelOpponent(baseline_model))\n",
        "model.set_env(self_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "learn(model, \"play_agenst_baseline\", self_env, 5_000_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(model_dir_for(\"play_agenst_baseline\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mload_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplay_agenst_baseline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[37], line 46\u001b[0m, in \u001b[0;36mload_and_evaluate\u001b[0;34m(name, env, config, iterations)\u001b[0m\n\u001b[1;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39mset_env(env)\n\u001b[1;32m     44\u001b[0m env\u001b[38;5;241m.\u001b[39menable_training()\n\u001b[0;32m---> 46\u001b[0m \u001b[43mEvaluationRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpygame_renderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/projects/asm/acp-demo/packages/pong-simulation/src/services/evaluation_runner.py:60\u001b[0m, in \u001b[0;36mEvaluationRunner.evaluate\u001b[0;34m(self, model, env, config, iterations)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Render the current state of the simulation, via the environment\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevents\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrewards\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m frame_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     65\u001b[0m result \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m~/projects/asm/acp-demo/packages/pong-simulation/src/renderer/pygame_renderer.py:132\u001b[0m, in \u001b[0;36mPygameRenderer.render\u001b[0;34m(self, state, events, rewards, config, frame_index)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_surface()\u001b[38;5;241m.\u001b[39mblit(score_text, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle_x \u001b[38;5;241m-\u001b[39m score_text\u001b[38;5;241m.\u001b[39mget_rect()\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m    131\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "load_and_evaluate(\"play_agenst_baseline\", self_env, env_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = load_model(\"play_agenst_baseline\")\n",
        "baseline_model = load_baseline_model()\n",
        "\n",
        "# still no reward for winning, we need to defend a moving target first\n",
        "reward_config = RewardConfig(\n",
        "    win_round = 5,\n",
        "    lose_round = -10,\n",
        "    paddle_hit = 5,\n",
        "    near_miss_multiplier = 0,\n",
        "    near_miss_exponent = 5,\n",
        "    near_miss_min_distance = height / 10,\n",
        "    survival_reward_multiplier = 5,\n",
        "    endurance_penalty_multiplier = -2,\n",
        ")\n",
        "\n",
        "env_config = EnvConfig(\n",
        "    paddle_l = player_paddle,\n",
        "    paddle_r = baseline_paddle,\n",
        "    width = width,\n",
        "    height = height,\n",
        ")\n",
        "\n",
        "self_env = PongEnv(env_config, reward_config)\n",
        "self_env.enable_training()\n",
        "self_env.set_opponent(ModelOpponent(baseline_model))\n",
        "model.set_env(self_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "learn(model, \"play_agenst_attack_baseline\", self_env, 1_000_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(model_dir_for(\"play_agenst_attack_baseline\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mload_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplay_agenst_attack_baseline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[37], line 46\u001b[0m, in \u001b[0;36mload_and_evaluate\u001b[0;34m(name, env, config, iterations)\u001b[0m\n\u001b[1;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39mset_env(env)\n\u001b[1;32m     44\u001b[0m env\u001b[38;5;241m.\u001b[39menable_training()\n\u001b[0;32m---> 46\u001b[0m \u001b[43mEvaluationRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpygame_renderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/projects/asm/acp-demo/packages/pong-simulation/src/services/evaluation_runner.py:60\u001b[0m, in \u001b[0;36mEvaluationRunner.evaluate\u001b[0;34m(self, model, env, config, iterations)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Render the current state of the simulation, via the environment\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevents\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrewards\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m frame_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     65\u001b[0m result \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m~/projects/asm/acp-demo/packages/pong-simulation/src/renderer/pygame_renderer.py:119\u001b[0m, in \u001b[0;36mPygameRenderer.render\u001b[0;34m(self, state, events, rewards, config, frame_index)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_endurance(state\u001b[38;5;241m.\u001b[39mpaddle_l, state\u001b[38;5;241m.\u001b[39mpaddle_l\u001b[38;5;241m.\u001b[39mmax_endurance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_endurance(state\u001b[38;5;241m.\u001b[39mpaddle_r, state\u001b[38;5;241m.\u001b[39mpaddle_r\u001b[38;5;241m.\u001b[39mmax_endurance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_rewards\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# self._render_ruler_y()\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# for wall in self.walls:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#     wall.draw(self.display)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# draw middle line\u001b[39;00m\n\u001b[1;32m    125\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mline(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_surface(), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle_x,\n\u001b[1;32m    126\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbottom), (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop), \u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/projects/asm/acp-demo/packages/pong-simulation/src/renderer/pygame_renderer.py:84\u001b[0m, in \u001b[0;36mPygameRenderer._render_rewards\u001b[0;34m(self, rewards)\u001b[0m\n\u001b[1;32m     82\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:  $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfont\u001b[38;5;241m.\u001b[39mrender(text, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLACK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_surface\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m line \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "load_and_evaluate(\"play_agenst_attack_baseline\", self_env, env_config)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "29287708f5ac4062cbd3fc6f78e4cc0472b9f1dcb960699e48edd5b82bb44a9e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}