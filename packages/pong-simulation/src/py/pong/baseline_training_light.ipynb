{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To create a virtual environment and install dependencies, run: `pants venv`. This creates a virtual environment with all the needed deps. Then activate the virtual environment by runnin `dist/export/python/virtualenvs/python-default/3.8.16/bin/source`.\n",
        "\n",
        "The next few blocks set-up python paths on dependent modules and imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"../../schemas/gen/py\")\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.1.2 (SDL 2.0.18, Python 3.8.16)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ],
      "source": [
        "from gym.wrappers import TimeLimit\n",
        "import gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo.policies import MultiInputPolicy\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common import results_plotter\n",
        "from tqdm.rich import trange\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "from pong.env.rewards import RewardConfig\n",
        "from pong.env.pong_env import PongEnv, EnvConfig, PaddleConfig\n",
        "from pong.env.opponent import StaticOpponent, ModelOpponent\n",
        "from pong.env.spaces import ActionType\n",
        "from pong.renderer.pygame_renderer import PygameRenderer\n",
        "from pong.renderer.event_renderer import EventRenderer, FileEventWriter\n",
        "import pong.services.evaluation_runner import EvaluationRunner, WrappedEnv\n",
        "from pong.simulation.config import SimulationConfig\n",
        "from pong.env.additional_logs_callback import AdditionalLogsCallback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Shared variables and utils\n",
        "Execute this block to allow partial execution of the notebook from a cell bellow\n",
        "\n",
        "### Baseline player config\n",
        "\n",
        "The baseline player has genome attributes that puts it at an advantage over most other as we want a good performing agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shared variables and configurations that could be used downstream. We co-locate these so this block could be executed\n",
        "# and partial execution from a cell bellow works.\n",
        "\n",
        "width = 800\n",
        "height = 400\n",
        "\n",
        "# Change this to use a clean output dir\n",
        "experiment_id = 1\n",
        "experiment = f\"baseline_light/{experiment_id}\"\n",
        "\n",
        "\n",
        "\n",
        "log_dir = f\".logs/{experiment}\"\n",
        "tb_log_dir = f\".logs/{experiment}_tb\"\n",
        "\n",
        "os.makedirs(experiment, exist_ok = True)\n",
        "os.makedirs(tb_log_dir, exist_ok = True)\n",
        "os.makedirs(f\".models/{experiment}\", exist_ok = True)\n",
        "\n",
        "\n",
        "def pygame_renderer() -> PygameRenderer:\n",
        "    return PygameRenderer(width, height, 60)\n",
        "\n",
        "def model_dir_for(name: str) -> str:\n",
        "    return f\".models/{experiment}/{name}\"\n",
        "\n",
        "def load_model(name: str) -> PPO:\n",
        "    model = PPO.load(model_dir_for(name))\n",
        "    model.tensorboard_log = tb_log_dir\n",
        "    return model\n",
        "\n",
        "def load_baseline_model() -> PPO:\n",
        "    return PPO.load(\"../models/balanced_opponent.zip\")\n",
        "\n",
        "def load_baseline_collab_model() -> PPO:\n",
        "    return PPO.load(\"../models/balanced_collab_opponent.zip\")\n",
        "\n",
        "def evaluate(model: PPO, env: gym.Env, config: SimulationConfig, iterations = 100):\n",
        "    EvaluationRunner(pygame_renderer).evaluate(model, env, config, 100)\n",
        "    \n",
        "def load_and_evaluate(name: str, env: PongEnv, config: SimulationConfig, iterations = 100):\n",
        "    model = load_model(name)\n",
        "    model.set_env(env)\n",
        "    env.enable_training()\n",
        "\n",
        "    EvaluationRunner(pygame_renderer()).evaluate(model, env, config, 100)\n",
        "    \n",
        "def load_and_evaluate_models(name_l: str, name_r: str, env: PongEnv, config: SimulationConfig, iterations = 100):\n",
        "    model_l = load_model(name_l)\n",
        "    model_r = load_model(name_r)\n",
        "\n",
        "    env.set_opponent(ModelOpponent(model_r))\n",
        "    env.enable_training()\n",
        "#     model_l.set_env(env)\n",
        "\n",
        "    EvaluationRunner(pygame_renderer()).evaluate(model_l, env, config, 100)\n",
        "\n",
        "# helper for running the training loop but keep checkpointing along the way\n",
        "def learn(model: PPO, name: str, env: PongEnv, iterations: int):\n",
        "    wrapped = TimeLimit(env, 60 * 60 * 1)\n",
        "    model.set_env(wrapped)\n",
        "    \n",
        "    # store every 10k steps\n",
        "    for r in range(0, iterations, 10_000):\n",
        "        model.learn(total_timesteps=10_000, tb_log_name=name, callback=AdditionalLogsCallback(), reset_num_timesteps=False)\n",
        "        model.save(model_dir_for(name))\n",
        "    \n",
        "    \n",
        "    \n",
        "# =========== Baseline player config ==============\n",
        "# The baseline player has genome attributes that puts it at an advantage over most other as we want a\n",
        "# good performing agent. \n",
        "player_paddle = PaddleConfig(width = 50, strength = 10, endurance=5, max_speed=2000,)\n",
        "\n",
        "# A static paddle that does not move\n",
        "wall_paddle = PaddleConfig(width = 1000, strength = 1000, endurance=3, max_speed=500,)\n",
        "\n",
        "# The baseline paddle trained with a better than usual genome.\n",
        "baseline_paddle = PaddleConfig(width = 100, strength = 25, endurance=10, max_speed=1000,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Play against the wall\n",
        "\n",
        "Start with a completely random model and train against a wall as we don't have any other baseline, This way the agents learns to follow the ball and defend on the early stages.\n",
        "Reward for near misses as this is a faster way for the agent to learn to "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "reward_config = RewardConfig(\n",
        "    win_round = 10,\n",
        "    lose_round = -5,\n",
        "    paddle_hit = 5,\n",
        "    near_miss_multiplier = 1,\n",
        "    near_miss_exponent = 5,\n",
        "    near_miss_min_distance = height / 2,\n",
        "    survival_reward_multiplier = 4,\n",
        "    endurance_penalty_multiplier = -1,\n",
        ")\n",
        "\n",
        "play_against_wall_config = EnvConfig(\n",
        "    paddle_l = player_paddle,\n",
        "    paddle_r = wall_paddle,\n",
        "    width = width,\n",
        "    height = height,\n",
        ")\n",
        "\n",
        "play_against_wall_env = PongEnv(play_against_wall_config, reward_config)\n",
        "play_against_wall_env.enable_training()\n",
        "play_against_wall_env.set_opponent(StaticOpponent(ActionType.STOP, 0.0))\n",
        "\n",
        "model = PPO(MultiInputPolicy, play_against_wall_env, verbose=0, device=\"cpu\", tensorboard_log=tb_log_dir, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "learn(model, \"wall_near_miss\", play_against_wall_env, 1_000_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(model_dir_for(\"wall_near_miss\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load_and_evaluate(\"wall_near_miss\", play_against_wall_env, play_against_wall_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Play against itself to improve on defense and learn attacking moves\n",
        "\n",
        "The use the same model to play with it-self. The wall helps to learn defense but does not allow to learn attacking strategies. Lest setup two reward confics so we could get two model behaviors, one model that is rewarded for defense and another to attack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = load_model(\"wall_near_miss\")\n",
        "baseline_model = load_baseline_collab_model()\n",
        "\n",
        "# still no reward for winning, we need to defend a moving target first\n",
        "reward_config = RewardConfig(\n",
        "    win_round = 10,\n",
        "    lose_round = -10,\n",
        "    paddle_hit = 5,\n",
        "    near_miss_multiplier = 0,\n",
        "    near_miss_exponent = 5,\n",
        "    near_miss_min_distance = height / 10,\n",
        "    survival_reward_multiplier = 2,\n",
        "    endurance_penalty_multiplier = -1,\n",
        ")\n",
        "\n",
        "env_config = EnvConfig(\n",
        "    paddle_l = player_paddle,\n",
        "    paddle_r = baseline_paddle,\n",
        "    width = width,\n",
        "    height = height,\n",
        ")\n",
        "\n",
        "self_env = PongEnv(env_config, reward_config)\n",
        "self_env.enable_training()\n",
        "self_env.set_opponent(ModelOpponent(baseline_model))\n",
        "model.set_env(self_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "learn(model, \"play_agenst_baseline\", self_env, 5_000_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(model_dir_for(\"play_agenst_baseline\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load_and_evaluate(\"play_agenst_baseline\", self_env, env_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = load_model(\"play_agenst_baseline\")\n",
        "baseline_model = load_baseline_model()\n",
        "\n",
        "# still no reward for winning, we need to defend a moving target first\n",
        "reward_config = RewardConfig(\n",
        "    win_round = 5,\n",
        "    lose_round = -10,\n",
        "    paddle_hit = 5,\n",
        "    near_miss_multiplier = 0,\n",
        "    near_miss_exponent = 5,\n",
        "    near_miss_min_distance = height / 10,\n",
        "    survival_reward_multiplier = 5,\n",
        "    endurance_penalty_multiplier = -2,\n",
        ")\n",
        "\n",
        "env_config = EnvConfig(\n",
        "    paddle_l = player_paddle,\n",
        "    paddle_r = baseline_paddle,\n",
        "    width = width,\n",
        "    height = height,\n",
        ")\n",
        "\n",
        "self_env = PongEnv(env_config, reward_config)\n",
        "self_env.enable_training()\n",
        "self_env.set_opponent(ModelOpponent(baseline_model))\n",
        "model.set_env(self_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "learn(model, \"play_agenst_attack_baseline\", self_env, 1_000_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(model_dir_for(\"play_agenst_attack_baseline\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_and_evaluate(\"play_agenst_attack_baseline\", self_env, env_config)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "29287708f5ac4062cbd3fc6f78e4cc0472b9f1dcb960699e48edd5b82bb44a9e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}